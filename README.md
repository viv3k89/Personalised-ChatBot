# Personalised-ChatBot
# ğŸ§  Personalized Chatbot using Ollama + Mistral 7B + Gradio

This project is a personalized chatbot built using the [Mistral 7B](https://mistral.ai/news/introducing-mistral-7b/) model via [Ollama](https://ollama.com), with a frontend interface created and deployed using [Gradio](https://www.gradio.app/).

## ğŸ“ Project Structure

.
â”œâ”€â”€ .gradio/ # Gradio-related config (e.g., state, sessions)
â”œâ”€â”€ frontend/ # HTML/CSS frontend files (optional customization)
â”œâ”€â”€ chatbot.py # Main chatbot logic
â”œâ”€â”€ chatbotdeployment.py # Gradio deployment script
â”œâ”€â”€ data.json # Sample or training data
â”œâ”€â”€ style.css # Custom styling for Gradio app
â”œâ”€â”€ txt # Additional static resources or logs

markdown
Copy
Edit

## ğŸš€ Features

- Uses **Mistral 7B** model for intelligent responses
- Runs locally via **Ollama**
- Clean and simple UI with **Gradio**
- Easy to customize and extend
- Can be deployed for local or web-based usage

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/personalized-chatbot.git
cd personalized-chatbot
2. Install Dependencies
Ensure Python 3.8+ is installed. Then:

bash
Copy
Edit
pip install -r requirements.txt
You may need to create a requirements.txt file with:

txt
Copy
Edit
gradio
requests
3. Install and Run Ollama with Mistral
Make sure you have Ollama installed and running. Then pull the Mistral model:

bash
Copy
Edit
ollama pull mistral
4. Run the Chatbot
bash
Copy
Edit
python chatbotdeployment.py
This will launch the Gradio interface in your browser.

ğŸ§  Model Details
Model: Mistral 7B

Provider: Ollama

Inference: Local LLM execution via Ollama's API

ğŸ’» Example
Once the app is running, you can start chatting:

makefile
Copy
Edit
User: Hello!
Bot: Hi there! How can I assist you today?
ğŸ“Œ Notes
Make sure Ollama is running in the background before launching the bot.

Customize style.css for UI styling.

Use data.json to load or save past conversations or context.

---

